{"cells":[{"cell_type":"markdown","source":["#Bank direct marketing prediction using Logistic Regression\n\nThe data is related with direct marketing campaigns of a Portuguese banking institution.The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed."],"metadata":{}},{"cell_type":"markdown","source":["## Objective\n\nThe classification goal is to predict if the client will subscribe a term deposit (variable result).\n\n\nInput data variables are as follows:\n## Bank client data:\n\n1 - age (numeric) \n\n2 - job : type of job \n\n3 - marital : marital status \n\n5 - default: has credit in default? \n\n6 - housing: has housing loan? \n\n7 - loan: has personal loan? \n\n## Related with the last contact of the current campaign:\n\n8 - contact: contact communication type \n\n9 - month: last contact month of year\n\n10 - day_of_week: last contact day of the week\n\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n## other attributes:\n\n12 - campaign: number of contacts performed during this campaign and for this client\n\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n## social and economic context attributes\n16 - emp_var_rate: employment variation rate - quarterly indicator (numeric)\n\n17 - cons_price_idx: consumer price index - monthly indicator (numeric) \n\n18 - cons_conf_idx: consumer confidence index - monthly indicator (numeric) \n\n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n\n20 - no_employed: number of employees - quarterly indicator (numeric)"],"metadata":{}},{"cell_type":"markdown","source":["# Data download"],"metadata":{}},{"cell_type":"code","source":["%sh\n\nwget --no-check-certificate 'https://onedrive.live.com/download?cid=EA5ED07A88AEEA57&resid=EA5ED07A88AEEA57%2115521&authkey=ALUhyTQc2xMJwas' -O bank.csv \n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import lit\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\nfrom pyspark.sql.functions import *  \nimport numpy as np # library for working with Arrays\nimport pandas as pd # makes working with data tables easier\nimport matplotlib.pyplot as plt # module for plotting\nimport seaborn as sns #"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["bankDF = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option('inferSchema', 'true')\\\n  .load(\"file:/databricks/driver/bank.csv\")\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Data explored and explained"],"metadata":{}},{"cell_type":"code","source":["display(bankDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Data cleaning"],"metadata":{}},{"cell_type":"code","source":[" from pyspark.ml import Pipeline\n from pyspark.ml.classification import LogisticRegression\n from pyspark.ml.feature import HashingTF, Tokenizer\n from pyspark.sql import Row\n from pyspark.sql.functions import UserDefinedFunction\n from pyspark.sql.types import *"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["bankDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Remove columns with bad (null) ID or bad result column\nbank_cleaned = bankDF.filter(bankDF.age.isNotNull() & bankDF.duration.isNotNull() & bankDF.campaign.isNotNull() & bankDF.pdays.isNotNull() & bankDF.previous.isNotNull() & bankDF.duration.isNotNull() & bankDF.emp_var_rate.isNotNull() & bankDF.cons_price_idx.isNotNull() & bankDF.cons_conf_idx.isNotNull() & bankDF.euribor3m.isNotNull() & bankDF.no_employed.isNotNull())\ndisplay(bank_cleaned)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["By exploring the data, here we see that people who are more than 21yrs have subscribed to the product."],"metadata":{}},{"cell_type":"code","source":["bank_cleaned.select('age','result').distinct().show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#bank_cleaned.write.saveAsTable('bank') "],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#Drop the table\n\n#%sql\n#drop table bank"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql\nSELECT duration,poutcome,result FROM bank GROUP BY duration,poutcome,result ORDER BY duration desc"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sql\nSELECT campaign, COUNT(campaign) AS cnt FROM bank GROUP BY campaign"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Data transformation"],"metadata":{}},{"cell_type":"code","source":["# Convert results for to MLlib input, which requires labels as a float\ndef labelForResults(s):\n     if s == 'yes':\n         return 1.0\n     else:\n         return 0.0\n\nlabel = UserDefinedFunction(labelForResults, DoubleType())  \nlabeledData = bank_cleaned.select(label(bank_cleaned.result).alias('label'),bank_cleaned.duration).where('label >= 0')\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(labeledData)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["reduced_numeric_cols = ['age',\n                          'duration',\n                          'campaign',\n                          'pdays',\n                          'previous',\n                          'emp_var_rate',\n                          'cons_price_idx',\n                          'cons_conf_idx'\n                         ]"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#reduced_numeric_cols.show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\n\nlabel_indexer = StringIndexer(inputCol = 'result', outputCol = 'label')\nassembler = VectorAssembler(\n    inputCols = reduced_numeric_cols,\n    outputCol = 'features')"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Data modeling"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\n\nclassifier = LogisticRegression(regParam=0.01, labelCol = 'label', featuresCol = 'features')\n\npipeline = Pipeline(stages=[label_indexer, assembler, classifier])\n\n(train, test) = bank_cleaned.randomSplit([0.7, 0.3])\nmodel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["display(train)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\npredictions = model.transform(test)\nevaluator = BinaryClassificationEvaluator()\nauroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\naupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\nraw = evaluator.evaluate(predictions)\n\"The AUROC is %s and the AUPR is %s and raw is %s.\" % (auroc, aupr, raw)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Prediction"],"metadata":{}},{"cell_type":"markdown","source":["## Model evaluation"],"metadata":{}},{"cell_type":"code","source":["Successes = predictions.where(\"(label = 0 AND prediction = 0) OR  (label = 1 AND prediction = 1)\").count()\nResult = predictions.count()\n\nprint \"There were\", Result, \"outputs and there were\", Successes, \"successful predictions\"\nprint \"This is a\", str((float(Successes) / float(Result)) * 100) + \"%\", \"success rate\""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## Visualization"],"metadata":{}},{"cell_type":"code","source":["truePositive = int(predictions.where(\"(label = 1 AND prediction = 1)\").count())\ntrueNegative = int(predictions.where(\"(label = 0 AND prediction = 0)\").count())\nfalsePositive = int(predictions.where(\"(label = 0 AND prediction = 1)\").count())\nfalseNegative = int(predictions.where(\"(label = 1 AND prediction = 0)\").count())\n\nprint [['TP', truePositive], ['TN', trueNegative], ['FP', falsePositive], ['FN', falseNegative]]\nresultDF = sqlContext.createDataFrame([['TP', truePositive], ['TN', trueNegative], ['FP', falsePositive], ['FN', falseNegative]], ['metric', 'value'])\ndisplay(resultDF)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"name":"Banking","notebookId":3553130718193668},"nbformat":4,"nbformat_minor":0}
